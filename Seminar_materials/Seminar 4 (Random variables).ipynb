{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f7b639",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Seminar 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-example",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random variables\n",
    "\n",
    "A **random variable** is a function from sample space to the real numbers $X: S \\to \\mathbb{R}$.\n",
    "\n",
    "It means that for every outcome $\\omega \\in S$ there is a real number $X(\\omega)$.\n",
    "\n",
    "Not all functions are allowed, but this is slightly beyond the scope of our course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-newton",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distribution of a random variable\n",
    "\n",
    "Consider random variable $X: S \\to \\mathbb{R}$. We introduce **the distribution** (or distribution law) $\\mathcal{L}$ of random variable $X$. Distribution acts on numbers in $\\mathbb{R}$ in the same way as probability function $P$ acts on outcomes. We will write $X \\sim \\mathcal{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-hospital",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two types of distributions\n",
    "\n",
    "A probability distribution can be **discrete** or **continuous**.\n",
    "\n",
    "Discrete random variables can only take countably many values (think integers), continuous random variables can take uncountably many values (think reals).\n",
    "\n",
    "There is also a third type of distributions, which you never encounter in practice; it's possible for a distribution to be a mix of several types, which you also do not normally encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-hamilton",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 1\n",
    "\n",
    "Consider event $A$ and a random variable $X = \\mathbb{I}\\text{nd}_A$, an indicator:\n",
    "$$\n",
    "\\mathbb{I}\\text{nd}_A(x) = \\begin{cases}\n",
    "1, x \\in A, \\\\\n",
    "0, \\text{else}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X = 1) = \\mathbb{P}(A) = p\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X = 0) = 1 - \\mathbb{P}(A) = 1 - p\n",
    "$$\n",
    "\n",
    "We say that $X$ follows **Bernoulli distribution** with parameter $p$ and write $X \\sim Be(p)$.\n",
    "\n",
    "We will call $\\mathbb{P}_X(\\omega)$ a **probability mass function** (PMF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-connection",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bernoulli trial scheme\n",
    "\n",
    "Previously we have worked with independent events that were happening in one probability space. But sometimes we want to have multiple trials, where for every trial the probability space is known, but we are interested in the probability space covering all the trials at once. We can achieve it via direct product of probability spaces.\n",
    "\n",
    "If all probability spaces are the same and equal to:\n",
    "- $S = \\{0, 1\\}$\n",
    "- $\\mathbb{P}(1) = p$ and $\\mathbb{P}(0) = 1 - p$\n",
    "\n",
    "Then we call such experiment a **Bernoulli trial scheme**, and the probability space of it is:\n",
    "- $S = \\{(i_1, \\ldots, i_n), i_j \\in \\{0, 1\\}\\}$\n",
    "- $\\mathbb{P}(i_1, \\ldots, i_n) = p^{\\text{num} j \\text{ such that } i_j = 1} (1 - p)^{\\text{num} j \\text{ such that } i_j = 0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-league",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2\n",
    "\n",
    "Consider $X_1, \\ldots, X_n \\sim Be(p)$ independent random variables. Then $Y = \\sum_{k=1}^n X_k$ follows **Binomial distribution** with parameters $n$ and $p$, $Y \\sim Bi(n, p)$. $\\mathbb{P}(Y = k) = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-clinton",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution 2\n",
    "\n",
    "If $Y \\sim Bi(n, p)$, then\n",
    "$$\n",
    "\\mathbb{P}(Y = k) = \\begin{pmatrix}n\\\\k\\end{pmatrix} p^k (1-p)^{n-k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394172b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 3\n",
    "\n",
    "We say $X$ follows discrete uniform distribution $DU([1, n])$ and we write $X \\sim DU([1, n])$ if\n",
    "$$\n",
    "P(X = k) = \\frac1n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a845a49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 4\n",
    "\n",
    "Consider an urn with $w$ white balls and $b$ black balls. We draw $n$ balls out of the urn at random without replacement. Let $X$ be the number of white balls in the sample. What is the distribution of $X$? What is its PMF?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad5a285",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution 4\n",
    "\n",
    "If $X \\sim HGeom(w, b, n)$, then\n",
    "$$\n",
    "\\mathbb{P}(X = k) = \\frac{\\begin{pmatrix}w\\\\k\\end{pmatrix}\\begin{pmatrix}b\\\\n-k\\end{pmatrix}}{\\begin{pmatrix}w+b\\\\n\\end{pmatrix}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70216d6f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 5\n",
    "\n",
    "What is the difference between hypergeometric and binomial distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0414680",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reminder:\n",
    "- Binomial story: Consider and urn with $w$ white balls and $b$ black balls. We draw $n$ balls from the urn with replacement. Let $X$ be the number of white balls in the sample.\n",
    "- Hypergeometric story: Consider an urn with $w$ white balls and $b$ black balls. We draw $n$ balls out of the urn at random without replacement. Let $X$ be the number of white balls in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9680151",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Bernoulli trials in Binomial story are independent. The Bernoulli trials in the Hypergeometric story are dependent, since the sampling is done without replacement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-donor",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 5\n",
    "\n",
    "Consider $X$ and $Y$ independent $\\mathbb{Z}$-valued random variables. $\\mathbb{P}(X + Y = k) = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-chancellor",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution 5\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(X + Y = k) = \\sum_{m} \\mathbb{P}(X = m) \\mathbb{P}(Y = k - m)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-college",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 6\n",
    "\n",
    "Let $X \\sim Bi(n, p)$ and $Y \\sim Bi(m, p)$ be independent. What is the distribution of $Z = X + Y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-member",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution 6\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}(X + Y = k) & = \\sum_j \\begin{pmatrix}n\\\\j\\end{pmatrix} p^j (1-p)^{n-j} \\begin{pmatrix}m\\\\k-j\\end{pmatrix} p^{k-j} (1-p)^{m-k+j} = \\\\\n",
    "& = p^{k} (1-p)^{n+m-k} \\sum_j \\begin{pmatrix}n\\\\j\\end{pmatrix} \\begin{pmatrix}m\\\\k-j\\end{pmatrix} = \\\\\n",
    "& =\\begin{pmatrix}n+m\\\\k\\end{pmatrix} p^{k} (1-p)^{n+m-k}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z \\sim Bi(n+m, p)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-delaware",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cumulative distribution function\n",
    "\n",
    "Note that distribution of a discrete distribution of a random variable $X$ is uniquely defined by its PMF $\\mathbb{P}(X = x_i)$. In general, we define the distribution using cumulative distribution function (CDF):\n",
    "$$\n",
    "F_X(x) = \\mathbb{P}(X < x)\n",
    "$$\n",
    "\n",
    "It has the following properties:\n",
    "- $F_X$ is non-decreasing\n",
    "- $\\lim\\limits_{x\\to-\\infty}F_X(x) = 0$\n",
    "- $\\lim\\limits_{x\\to+\\infty}F_X(x) = 1$\n",
    "- $F_X$ if left continuous\n",
    "\n",
    "Interesting enough, the converse is also true. Any function that conforms to the properties above defines some probability distribution on $\\mathbb{R}$ and this relation is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-haiti",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probability density function\n",
    "\n",
    "- If $X$ has a discrete distribution, then $F_X$ has a countable number of jumps $p_i = \\mathbb{P}(X = x_i)$ and at $x = x_i$ it is continuous\n",
    "- If $X$ has absolutely continuous distribution, then $F_X$ is differentiable a.e. and can be recovered from its derivative:\n",
    "    $$\n",
    "    F_X(x) = \\int\\limits_{-\\infty}^x f_X(t) dt\n",
    "    $$\n",
    "    \n",
    "    where $f_X(t)$ is the probability density function and $f_X(t) = F'_X(x)$ a.e."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-chinese",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 7\n",
    "\n",
    "We say that random variable $X$ is distributed uniformly on $[a, b]$ and write $X \\sim U([a, b])$ if\n",
    "$$\n",
    "f_X(x) = \\begin{cases}\n",
    "\\frac{1}{b-a}, a \\leqslant x \\leqslant b, \\\\\n",
    "0, \\text{else}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-acrylic",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 8\n",
    "\n",
    "Consider $X$ and $Y$ independent random variables with PDFs $f_X$ and $f_Y$ respectively. Then, their sum $Z = X + Y$ has absolutely continuous distribution with density\n",
    "$$\n",
    "f_Z(z) = \\int f_X(x) f_Y(z-x) dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-minnesota",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 9\n",
    "\n",
    "Let $X, Y \\sim U([0, 1])$ and $Z = X + Y$. Find $f_Z(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-commerce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Solution 9\n",
    "\n",
    "$$\n",
    "f_Z(z) = \\int\\limits_{0}^1 f_X(x) f_Y(z-x) dx = \\int\\limits_{0}^1 f_Y(z-x) dx = \\begin{cases}\n",
    "z, & 0 \\leqslant z \\leqslant 1, \\\\\n",
    "2 - z, & 1 \\leqslant z \\leqslant  2, \\\\\n",
    "0, & \\text{else}\n",
    "\\end{cases}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
